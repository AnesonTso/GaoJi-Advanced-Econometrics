{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2 Random Variables Part VI\n",
    "\n",
    "#### *Zhuo Jianchao* \n",
    "\n",
    "Feb 16, 2020 *Rev 1*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moment Generating Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've known the definition of moments, let's calculate one as practice, $\\sigma_{X}^2$.\n",
    "\n",
    "$$ \\begin{aligned}\n",
    "\\sigma_{X}^2 &= E[(X-\\mu_{X})^2]\\\\\n",
    "&= E[X^2-2X\\mu_{X}+\\mu_{X}^2]\\\\\n",
    "&= E(X^2)-2\\mu_{X}^2+\\mu_{X}^2\\\\\n",
    "&= E(X^2)-\\mu_{X}^2\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $E(X^2)=\\int_{-\\infty}^{+\\infty}x^2f_{X}(x)\\:dx$\n",
    "\n",
    "From this deduction, we have an another expression of the variance\n",
    "$$\\sigma_{X}^2=E(X^2)-\\mu_{X}^2$$\n",
    "\n",
    "We can see that *k*-th central moment can be induced from *k*-th moment, but when $f_{X}(x)$ becomes more complex, we may find increasingly difficult to calculate it, needless to say higher order of moments where there are higher terms for integral.\n",
    "\n",
    "To obtain higher order moments, we may turn to a technique called **moment generating function** which will bring calculative convenience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition 13 Moment Generating Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MGF of a random variable $X$ with PMF or PDF $f_{X}(x)$ is defined as\n",
    "$$\\begin{aligned}\n",
    "M_{X}(t) = E(e^{tX})\n",
    "  = \\begin{cases}\n",
    "  \\sum_{x \\in \\Omega_{X}}e^{tX}f_{X}(x) & \\text{if X is a DRV}, \\\\\n",
    "  \\int_{-\\infty}^{+\\infty}e^{tX}f_{X}(x)\\: dx & \\text{if X is a CRV}.\n",
    "    \\end{cases}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The MGF of $X$, $M_X(t)$ is the function of $t$ instead of $X$ since expectation takes in the expression of $X$ and results in a real value.\n",
    "\n",
    "***Theorem:*** The *k*-th derivative of $M_{X}(t)$ evaluated at $t=0$ is the *k*-th moment of $X$, that is,\n",
    "$$M_{X}^{(k)}(0)=E(X^k)$$\n",
    "\n",
    "Such a convenience to calculate *k*-th moment, but why?\n",
    "\n",
    "***Proof:*** We can expand $e^{tX}$ into power series with respect to $t$, as\n",
    "\n",
    "$$e^{tX} = 1+tX+\\frac{(tX)^2}{2!}+\\frac{(tX)^3}{3!}+\\cdots+\\frac{(tX)^k}{k!}+\\cdots = \\sum_{n=0}^{\\infty}\\frac{(tX)^n}{n!}.$$\n",
    "\n",
    "By the definition of MGF of $X$,\n",
    "\n",
    "$$\\begin{aligned}\n",
    "M_X(t) &= E(e^{tX})=E(1+tX+\\frac{(tX)^2}{2!}+\\cdots+\\frac{(tX)^k}{k!}+\\cdots)\\\\\n",
    "&= 1+tE(X)+\\frac{t^2E[X^2]}{2!}+\\cdots+\\frac{t^kE[X^t]}{k!}+\\cdots\\\\\n",
    "&= 1+\\mu_{X}t+\\frac{\\mu_2}{2!}t^2+\\cdots+\\frac{\\mu_k}{k!}t^k+\\cdots\\\\\n",
    "&= \\sum_{n=0}^{\\infty}\\frac{\\mu_n}{n!}t^n\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $\\mu_{k}$ stands for *k*-th moment of $X$.\n",
    "\n",
    "Taking *k*-th order derivative with respect to $t$, all terms of $t$ to the order less than *k* will be ruled out, which yields, \n",
    "$$M_{X}^{(k)}(t)=\\sum_{n=0}^{\\infty}\\mu_k t^n$$\n",
    "\n",
    "Taking into $t=0$, all terms which is multiplied by $t$ becomes zero, which yields\n",
    "$$M_{X}^{(k)}(0)=\\mu_k=E(X^k)$$\n",
    "\n",
    "**Notice** that\n",
    "1. MGF of $X$ may not even exist if $E(e^{tX})$ doesn't exist in the neighborhood of $t=0$. It happens in some cases where $M_{X}(0)$ is infinite or just undefined. \n",
    "2. If $M_{X}(t)$ exists in the neighborhood of $t=0$, it indicates any order of moment of $X$ exists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Properties of MGF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some properties of MGF which will benefit our construction of MGF of more complex random variable.\n",
    "\n",
    "#### Property 1 Linear Transformation\n",
    "\n",
    "Suppose $Y=a+bX$, where $a$ and $b$ are two constants, and the MGF of $X$, $M_{X}(t)$, exists in  the neighborhood of $t=0$, then the MGF of $Y$ is that\n",
    "$$M_{Y}(t)=e^{at}M_{X}(bt)$$\n",
    "\n",
    "***Proof:*** By the definition of $M_{Y}(t)$, and take into $Y=a+bX$, yields\n",
    "$$M_{Y}(t)=E(e^{tY})=E[e^{t(a+bX)}]\\\\=E[e^{at+btX}]\\\\=E[e^{at}e^{btX}]\\\\=e^{at}E[e^{btX}]\\\\=e^{at}M_{X}(bt)$$\n",
    "\n",
    "#### Property 2 Convolution\n",
    "\n",
    "Suppost $X$ and $Y$ are two independent random variable with MGF $M_{X}(t)$ and $M_{Y}(t)$ respectively. Then the MGF of $Z=X+Y$ is that\n",
    "$$M_Z(t)=M_X(t)M_Y(t)$$\n",
    "\n",
    "***Proof:*** By the definition of $M_{Z}(t)$,\n",
    "$$M_Z(t)=E(e^{tZ})\\\\=E[e^{t(X+Y)}]\\\\=E[e^{tX+tY}]\\\\=E[e^{tX}e^{tY}]$$\n",
    "By the property of the multiplication of two independent random variable, \n",
    "$$M_Z(t)=E[e^{tX}e^{tY}]=E[e^{tX}]E[e^{tY}]\\\\=M_{X}(t)M_{Y}(t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Characteristic Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We say a moment generating function of $X$, $M_X(t)$, exists as long as $M_{X}(t)$ is well defined at neighborhood of $t=1$, and it may not exist for some random variables. A more general function which is well defined at all $t$ for all random variables is **characteristic function**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition 14 Characteristic Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The characteristic function of a random variable $X$ with cumulative distribution function $F_{X}(x)$ is defined as\n",
    "$$\\begin{aligned}\n",
    "\\phi_X(t)&=E(e^{\\mathrm{i}tX})\\\\\n",
    "&=\\int_{-\\infty}^{+\\infty}e^{\\mathrm{i}tx}dF_{X}(x)\\\\\n",
    "&=\\int_{-\\infty}^{+\\infty}e^{\\mathrm{i}tx}f_{X}(x) \\: dx,\n",
    "\\end{aligned}$$\n",
    "where $\\mathrm{i}=\\sqrt{-1},$ and \n",
    "$$e^{\\mathrm{i}tx}=\\cos(tx)+\\mathrm{i}\\sin(tx)$$\n",
    "\n",
    "The characteristic function of $X$, $\\phi_X(t)$, is much similar to MGF of $X$, $M_{X}(t)$ in the following ways:\n",
    "\n",
    "***Theorem:*** Suppose *k*-th moment of $X$ exists. Then the *k*-th derivative of $\\phi_{X}(t)$ evaluated at $t=0$ is the *k*-th moment of $X$ multiplied by $\\mathrm{i}$ to the power of *k*, that is,\n",
    "$$\\phi_{X}^{(k)}(0)=\\mathrm{i}^k E(X^k)$$\n",
    "\n",
    "#### Property 1 Linear Transformation\n",
    "\n",
    "Suppose $Y=a+bX$, where $a$ and $b$ are two constants, and $X$ has the characteristic function $\\phi_X(t)$, then the characteristic function of $Y$ is that\n",
    "$$\\phi{Y}(t)=e^{\\mathrm{i}at}\\phi_{X}(bt)$$\n",
    "\n",
    "#### Property 2 Convolution\n",
    "\n",
    "Suppost $X$ and $Y$ are two independent random variable with characteristic function $\\phi_{X}(t)$ and $\\phi_{Y}(t)$ respectively. Then the characteristic function of $Z=X+Y$ is that\n",
    "$$\\phi_Z(t)=\\phi_X(t)\\phi_Y(t)$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.3.1",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
